{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfdd779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import sqlite3\n",
    "import voyageai\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "import concurrent.futures\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33162cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and texts from the database\n",
    "\n",
    "def load_embeddings_from_db(db_path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        if not os.path.exists(db_path):\n",
    "            raise FileNotFoundError(f\"Database not found: {db_path}\")\n",
    "            \n",
    "        conn = sqlite3.connect(db_path)\n",
    "        conn.row_factory = sqlite3.Row\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        cursor.execute(\"SELECT id, content, source, embedding FROM chunks\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        if not rows:\n",
    "            print(f\"Warning: No chunks found in {db_path}\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        records = []\n",
    "        for row in rows:\n",
    "            try:\n",
    "                # Convert BLOB back to numpy array\n",
    "                embedding = np.frombuffer(row[\"embedding\"], dtype=np.float32)\n",
    "                records.append({\n",
    "                    \"id\": row[\"id\"], \n",
    "                    \"document\": row[\"content\"], \n",
    "                    \"source\": row[\"source\"], \n",
    "                    \"embedding\": embedding\n",
    "                })\n",
    "            except (ValueError, TypeError) as e:\n",
    "                print(f\"Warning: Skipping invalid row {row['id']}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        df = pd.DataFrame(records)\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading embeddings from {db_path}: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        if 'conn' in locals():\n",
    "            conn.close()\n",
    "\n",
    "df_sql = load_embeddings_from_db(\"../databases/voyage_3_large_nice_guidelines.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21182adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6779893a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter unwanted guideline sections\n",
    "unwanted_patterns = [\n",
    "    'Putting this guideline into practice',\n",
    "    'Finding more information and committee details',\n",
    "    'Update information',\n",
    "    'Evaluation committee members and NICE project team',\n",
    "    'Committee members',\n",
    "    'Appendix',\n",
    "    'About this quality standard',\n",
    "    'Endorsing organisation',\n",
    "    'Supporting organisations',\n",
    "    'Diagnostics advisory committee members and NICE project team',\n",
    "    'Appraisal committee members',\n",
    "    'Sources of evidence',\n",
    "    r'(The\\s+)?[Cc]ommittee\\'?s?\\s+discussion' \n",
    "]\n",
    "\n",
    "filtered_df = df_sql[df_sql['source'].str.startswith('CG') | df_sql['source'].str.startswith('NG')]\n",
    "committee_pattern = r'(The\\s+)?[Cc]ommittee\\'?s?\\s+discussion'\n",
    "filtered_df = filtered_df[~filtered_df['id'].str.contains(committee_pattern, case=False, na=False, regex=True)]\n",
    "\n",
    "other_sections = [p for p in unwanted_patterns if not p.startswith('(')]\n",
    "for section in other_sections:\n",
    "    filtered_df = filtered_df[~filtered_df['id'].str.contains(section, case=False, na=False)]\n",
    "\n",
    "filtered_df = filtered_df[filtered_df['document'].str.len() >= 100]\n",
    "\n",
    "print(f\"Original rows: {len(df_sql)}\")\n",
    "print(f\"Filtered rows: {len(filtered_df)}\")\n",
    "filtered_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4597a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2 = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5d979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate queries for each chunk\n",
    "\n",
    "def generate_query_batch(rows, model=\"gpt-4.1-nano\"):\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        doc_id = row['id']\n",
    "        doc_content = row['document']\n",
    "        prompt = f\"\"\"Document Excerpt: {doc_content}\n",
    "                Generate a realistic search query for this NICE guideline content.\"\"\"\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"\"\"You are an expert assistant specialized in generating realistic search queries for NICE guidelines.\n",
    "\n",
    "                        Your task: Generate a natural question that a healthcare professional or patient would realistically use to find the given information.\n",
    "\n",
    "                        Requirements:\n",
    "                        - Start with question words: 'What', 'How', 'When', 'Should', 'Which', 'Why', etc.\n",
    "                        - Focus on the core clinical topic or specific recommendation\n",
    "                        - Use natural medical terminology that real users would search for\n",
    "                        - Keep it concise and directly related to the content\n",
    "                        - Return only the question, no additional formatting or explanations\n",
    "\n",
    "                        Examples:\n",
    "                        - \"What are the treatment options for managing hypertension in pregnant women?\"\n",
    "                        - \"How should blood glucose be monitored in diabetes patients?\"\n",
    "                        - \"When should antibiotics be prescribed for respiratory infections?\"\n",
    "                        - \"Which medications are recommended for chronic pain management?\"\n",
    "                        \"\"\"\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=0.3,\n",
    "                max_tokens=60,\n",
    "            )\n",
    "            query = response.choices[0].message.content.strip()\n",
    "            results[doc_id] = query\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred for document {doc_id}: {e}\")\n",
    "            results[doc_id] = \"NaN\"\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e85e454",
   "metadata": {},
   "source": [
    "# Batch run of query generation\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def batch_generate_queries(df, batch_size=100, model=\"gpt-4.1-nano\"):\n",
    "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    all_results = {}\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(generate_query_batch, batch.to_dict('records'), model) for batch in batches]\n",
    "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Generating queries\"):\n",
    "            all_results.update(future.result())\n",
    "    return all_results\n",
    "\n",
    "query_results = batch_generate_queries(filtered_df2, batch_size=20, model=\"gpt-4.1-nano\")\n",
    "filtered_df2[\"query\"] = filtered_df2['id'].map(query_results)\n",
    "filtered_df2.to_csv(\"databases/suggested_queries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df2 = pd.read_csv(\"databases/suggested_queries.csv\", index_col=False)\n",
    "filtered_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02143896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database with embeddings of generated questions using Voyage AI\n",
    "\n",
    "def get_voyage_client():\n",
    "    voyage_api_key = os.getenv(\"VOYAGE_API_KEY\")\n",
    "    return voyageai.Client(api_key=voyage_api_key)\n",
    "\n",
    "def get_embeddings_batch(texts, client, model=\"voyage-3.5\"):\n",
    "    try:\n",
    "        result = client.embed(\n",
    "            texts=texts,\n",
    "            model=model,\n",
    "            input_type=\"query\",\n",
    "            output_dimension=2048,\n",
    "            truncation=True\n",
    "        )\n",
    "        return result.embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        raise\n",
    "\n",
    "def init_query_database(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS queries (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            query TEXT NOT NULL,\n",
    "            query_embedding BLOB\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def store_query(conn, query_id, query, embedding):\n",
    "    cursor = conn.cursor()\n",
    "    embedding_blob = np.array(embedding, dtype=np.float32).tobytes() if embedding is not None else None\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO queries (id, query, query_embedding)\n",
    "        VALUES (?, ?, ?)\n",
    "    \"\"\", (query_id, query, embedding_blob))\n",
    "    conn.commit()\n",
    "\n",
    "def create_query_embeddings_db(csv_file, model_name=\"voyage-3.5\"):\n",
    "    db_path = f\"databases/{model_name.replace('-', '_')}_suggested_queries_2048.db\"\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    init_query_database(db_path)\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded {len(df)} queries from {csv_file}\")\n",
    "\n",
    "    voyage_client = get_voyage_client()\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing query batches\"):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        queries = batch[\"query\"].fillna(\"\").tolist()\n",
    "        ids = batch[\"id\"].tolist()\n",
    "        try:\n",
    "            embeddings = get_embeddings_batch(queries, voyage_client, model_name)\n",
    "            for query_id, query, embedding in zip(ids, queries, embeddings):\n",
    "                store_query(conn, query_id, query, embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"Query database creation/update completed: {db_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"databases/suggested_queries.csv\"\n",
    "    create_query_embeddings_db(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faabd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database with embeddings of generated questions using OpenAI\n",
    "\n",
    "filtered_df2 = pd.read_csv(\"databases/suggested_queries.csv\")\n",
    "\n",
    "def get_openai_client():\n",
    "    \"\"\"Get OpenAI client with API key from environment.\"\"\"\n",
    "    openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    return OpenAI(api_key=openai_api_key)\n",
    "\n",
    "def get_embeddings_batch(texts, client, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"Get embeddings for a batch of texts using OpenAI API.\"\"\"\n",
    "    try:\n",
    "        response = client.embeddings.create(\n",
    "            input=texts,\n",
    "            model=model\n",
    "        )\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting embeddings: {e}\")\n",
    "        raise\n",
    "\n",
    "def init_query_database(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS queries (\n",
    "            id TEXT PRIMARY KEY,\n",
    "            query TEXT NOT NULL,\n",
    "            query_embedding BLOB\n",
    "        )\n",
    "    ''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def store_query(conn, query_id, query, embedding):\n",
    "    cursor = conn.cursor()\n",
    "    embedding_blob = np.array(embedding, dtype=np.float32).tobytes() if embedding is not None else None\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT OR REPLACE INTO queries (id, query, query_embedding)\n",
    "        VALUES (?, ?, ?)\n",
    "    \"\"\", (query_id, query, embedding_blob))\n",
    "    conn.commit()\n",
    "\n",
    "def create_query_embeddings_db(csv_file, model_name=\"text-embedding-3-large\"):\n",
    "    db_path = f\"databases/{model_name.replace('-', '_')}_suggested_queries.db\"\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    init_query_database(db_path)\n",
    "\n",
    "    df = pd.read_csv(csv_file)\n",
    "    print(f\"Loaded {len(df)} queries from {csv_file}\")\n",
    "\n",
    "    openai_client = get_openai_client()\n",
    "    conn = sqlite3.connect(db_path)\n",
    "\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing query batches\"):\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        queries = batch[\"query\"].fillna(\"\").tolist()\n",
    "        ids = batch[\"id\"].tolist()\n",
    "        try:\n",
    "            embeddings = get_embeddings_batch(queries, openai_client, model_name)\n",
    "            for query_id, query, embedding in zip(ids, queries, embeddings):\n",
    "                store_query(conn, query_id, query, embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing batch starting at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    conn.close()\n",
    "    print(f\"Query database creation/update completed: {db_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_file = \"databases/suggested_queries.csv\"\n",
    "    create_query_embeddings_db(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ca242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test query database loading\n",
    "\n",
    "def load_query_database(db_path):\n",
    "    \"\"\"Load queries and embeddings from the database\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    conn.row_factory = sqlite3.Row  \n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    cursor.execute(\"SELECT id, query, query_embedding FROM queries\")\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    records = []\n",
    "    for row in rows:\n",
    "        embedding = None\n",
    "        if row[\"query_embedding\"]:\n",
    "            embedding = np.frombuffer(row[\"query_embedding\"], dtype=np.float32)\n",
    "        \n",
    "        records.append({\n",
    "            \"id\": row[\"id\"],\n",
    "            \"query\": row[\"query\"], \n",
    "            \"embedding\": embedding\n",
    "        })\n",
    "    \n",
    "    conn.close()\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "# Voyage 3.5 model\n",
    "df_queries = load_query_database(\"databases/voyage_3_5_suggested_queries.db\")\n",
    "\n",
    "# Voyage 3 Large model  \n",
    "# df_queries = load_query_database(\"databases/voyage_3_large_suggested_queries_2048.db\")\n",
    "\n",
    "# OpenAI text-embedding-3-large\n",
    "# df_queries = load_query_database(\"databases/text_embedding_3_large_suggested_queries.db\")\n",
    "\n",
    "print(f\"Loaded {len(df_queries)} queries\")\n",
    "df_queries.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

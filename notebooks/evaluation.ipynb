{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Path Setup ---\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == \"notebooks\":\n",
    "    project_root = current_dir.parent\n",
    "else:\n",
    "    project_root = current_dir\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "sys.path.insert(0, str(project_root / \"src\"))\n",
    "\n",
    "os.chdir(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb36d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "\n",
    "import ast\n",
    "import logging\n",
    "import voyageai\n",
    "import anthropic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.notebook import tqdm\n",
    "from src.query_rag import RAGSystem  \n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from src.query_rag_retrieval import RetrievalEvaluationSystem\n",
    "\n",
    "from ragas_modified.evaluation import evaluate\n",
    "from ragas_modified.run_config import RunConfig\n",
    "from ragas_modified.llms import LangchainLLMWrapper\n",
    "from ragas_modified.dataset_schema import EvaluationDataset\n",
    "from ragas_modified.embeddings import LangchainEmbeddingsWrapper\n",
    "from ragas_modified.metrics import LLMContextPrecisionWithReference, LLMContextRecall, Faithfulness, ResponseRelevancy\n",
    "\n",
    "# Disable logging for RAG system\n",
    "logging.disable(logging.CRITICAL)\n",
    "\n",
    "# Initialize Voyage AI client\n",
    "vo = voyageai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa44a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the testing dataset\n",
    "df_QA = pd.read_csv(\"data/NICEQA_utf8.csv\", encoding=\"utf-8\")\n",
    "df_QA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_QA[\"Answer\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f5c3b",
   "metadata": {},
   "source": [
    "## RAG Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbc366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for running RAG queries\n",
    "\n",
    "def query_rag_for_evaluation(query_text: str, model_weights: dict, rag_system: RAGSystem, llm_model: str = \"anthropic/claude-sonnet-4\",\n",
    "    similarity_k: int = 25, common_sections_n: int = 15, filename_type_filter: str = \"CG, NG\", info_source: str = \"NICE\", \n",
    "    use_hybrid_search: bool = True, wrrf_k: int = 40, use_reranker: bool = True, reranker_model: str = \"rerank-2\", reranker_top_k: int = 5\n",
    "):\n",
    "    \"\"\"Wrapper function to query the RAG system for evaluation.\"\"\"\n",
    "    try:\n",
    "        response_chunks = []\n",
    "        sources_string = \"\"\n",
    "        context_text = \"\"\n",
    "        \n",
    "        for chunk, chunk_sources, chunk_context, sources_data in rag_system.query_rag_stream(\n",
    "            query_text=query_text, llm_model=llm_model, similarity_k=similarity_k,\n",
    "            common_sections_n=common_sections_n, info_source=info_source,\n",
    "            model_weights=model_weights, filename_type_filter=filename_type_filter,\n",
    "            use_hybrid_search=use_hybrid_search, wrrf_k=wrrf_k,\n",
    "            use_reranker=use_reranker, reranker_model=reranker_model,\n",
    "            reranker_top_k=reranker_top_k\n",
    "        ):\n",
    "            response_chunks.append(chunk)\n",
    "            sources_string = chunk_sources\n",
    "            context_text = chunk_context\n",
    "        \n",
    "        final_response = ''.join(response_chunks)\n",
    "        \n",
    "        return final_response, sources_string, context_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in query_rag_for_evaluation: {e}\")\n",
    "        return f\"Error: {e}\", \"\", []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a8322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RAG system for each query in the dataset\n",
    "\n",
    "dataset = []\n",
    "sample_queries = df_QA[\"Question\"].tolist()\n",
    "expected_responses = df_QA[\"Answer\"].tolist()\n",
    "\n",
    "rag_system = RAGSystem()\n",
    "\n",
    "for query, reference in tqdm(zip(sample_queries, expected_responses), desc=\"Building evaluation dataset\", total=len(sample_queries)):\n",
    "\n",
    "    model_weights = {\"voyage-3-large\": 5.0, \"BM25\": 1.0}\n",
    "\n",
    "    response, sources, relevant_docs = query_rag_for_evaluation(query, rag_system=rag_system, model_weights=model_weights)\n",
    "\n",
    "    if not isinstance(relevant_docs, list):\n",
    "        relevant_docs = [relevant_docs]\n",
    "\n",
    "    dataset.append({\"user_input\": query, \"retrieved_contexts\": relevant_docs, \"response\": response, \"reference\": reference})\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "print(f\"Created evaluation dataset with {len(df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4392477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91242256",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"ragas_results2/rag_evaluation_dataset_claude_sonnet4_5chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23858c3e",
   "metadata": {},
   "source": [
    "## LLM-Only Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288c4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for LLM-only answers\n",
    "\n",
    "def llm_only_answer(query_text: str, llm_model: str = \"gpt-4.1-nano\", system_prompt: str | None = None, use_web_search: bool = False) -> str:\n",
    "    system_prompt = (\n",
    "        \"You are a medical AI assistant tasked with answering clinical questions strictly based on NICE clinical guidelines. \"\n",
    "        \"Follow these rules:\\n\"\n",
    "        \"1. Only use information from NICE guidelines.\\n\"\n",
    "        \"2. If no relevant NICE guideline information is available, reply: 'There are no relevant NICE guidelines for this request.'\\n\"\n",
    "        \"3. Be concise. Use markdown for lists and tables.\\n\"\n",
    "        \"4. Never fabricate sources or references.\\n\"\n",
    "    )\n",
    "    \n",
    "    if llm_model.startswith(\"claude\"):\n",
    "        if use_web_search:\n",
    "            anthropic_client = anthropic.Anthropic()\n",
    "            \n",
    "            full_query = f\"{system_prompt}\\n\\nUser question: {query_text}\"\n",
    "            \n",
    "            resp = anthropic_client.messages.create(\n",
    "                model=llm_model,\n",
    "                max_tokens=1024,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": full_query\n",
    "                    }\n",
    "                ],\n",
    "                tools=[{\n",
    "                    \"type\": \"web_search_20250305\",\n",
    "                    \"name\": \"web_search\",\n",
    "                    \"max_uses\": 5,\n",
    "                    \"allowed_domains\": [\"nice.org.uk\"]\n",
    "                }]\n",
    "            )\n",
    "            \n",
    "            if resp.content:\n",
    "                content_parts = []\n",
    "                for content in resp.content:\n",
    "                    if hasattr(content, 'text'):\n",
    "                        content_parts.append(content.text)\n",
    "                return '\\n'.join(content_parts) if content_parts else str(resp.content)\n",
    "            return str(resp)\n",
    "        else:\n",
    "            anthropic_client = OpenAI(\n",
    "                api_key=os.getenv(\"ANTHROPIC_API_KEY\"),\n",
    "                base_url=\"https://api.anthropic.com/v1/\"\n",
    "            )\n",
    "            resp = anthropic_client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": query_text},\n",
    "                ],\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "    else:\n",
    "        if \"o4-mini\" in llm_model:\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": query_text},\n",
    "                ]\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "        else:\n",
    "            client = OpenAI()\n",
    "            resp = client.chat.completions.create(\n",
    "                model=llm_model,\n",
    "                temperature=0,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": query_text},\n",
    "                ]\n",
    "            )\n",
    "            return resp.choices[0].message.content\n",
    "\n",
    "def make_query_embeddings(q: str):\n",
    "    vec = vo.embed(q, input_type=\"query\", model=\"voyage-3-large\", output_dimension=2048).embeddings\n",
    "    return {\"voyage-3-large\": np.array(vec, dtype=np.float32)}\n",
    "\n",
    "retrieval = RetrievalEvaluationSystem()\n",
    "\n",
    "def get_retrieved_context_texts(query: str, k: int = 15, n_common: int = 15, info_source: str = \"NICE\", filename_type_filter: str | None = None):\n",
    "    docs = retrieval.retrieve_documents(\n",
    "        query_embeddings=make_query_embeddings(query),\n",
    "        query_text=query, similarity_k=k,\n",
    "        common_sections_n=n_common, info_source=info_source,\n",
    "        model_weights={\"voyage-3-large\": 5.0, \"BM25\": 1.0}, filename_type_filter=filename_type_filter,\n",
    "        use_hybrid_search=True, use_reranker=True, reranker_model=\"rerank-2\",\n",
    "        reranker_top_k=10,\n",
    "        return_docs=True, \n",
    "    )\n",
    "    return [d.get(\"document\", \"\") for d in docs if d.get(\"document\")]\n",
    "\n",
    "llm_only_with_rag_contexts = []\n",
    "for query, reference in tqdm(zip(df_QA[\"Question\"], df_QA[\"Answer\"]), total=len(df_QA), desc=\"LLM-only + RAG contexts + Web Search\"):\n",
    "    contexts = get_retrieved_context_texts(query, k=15, n_common=15, info_source=\"NICE\")\n",
    "    response = llm_only_answer(query, llm_model=\"o4-mini\", use_web_search=False) \n",
    "    llm_only_with_rag_contexts.append({\"user_input\": query, \"retrieved_contexts\": contexts, \"response\": response, \"reference\": reference})\n",
    "\n",
    "df_llm_only = pd.DataFrame(llm_only_with_rag_contexts)\n",
    "print(f\"Created dataset with {len(df_llm_only)} samples\")\n",
    "df_llm_only.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e30c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm_only.to_csv(\"ragas_results2/rag_evaluation_dataset_o4_mini_baseline10_search.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3beb4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_llm_only[\"reference\"].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1701574c",
   "metadata": {},
   "source": [
    "## RAGAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset for evaluation\n",
    "df = pd.read_csv(\"baseline_results/rag_evaluation_dataset_gpt_4_1_nano_baseline10.csv\")\n",
    "df['retrieved_contexts'] = df['retrieved_contexts'].apply(ast.literal_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26ce422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up RAGAS evaluation\n",
    "openrouter_api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "evaluator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"openai/gpt-4.1-mini\", api_key=openrouter_api_key, base_url=\"https://openrouter.ai/api/v1/\"))\n",
    "evaluation_dataset = EvaluationDataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e14c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Evaluation\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  \n",
    "ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)\n",
    "#metrics = [LLMContextPrecisionWithReference(), LLMContextRecall(), ResponseRelevancy(), Faithfulness()]\n",
    "#metrics = [ResponseRelevancy(), Faithfulness()]\n",
    "metrics = [Faithfulness()]\n",
    "print(\"Starting RAGAS evaluation\")\n",
    "result = evaluate(dataset=evaluation_dataset, metrics=metrics, llm=evaluator_llm, embeddings=ragas_embeddings, raise_exceptions=True, run_config=RunConfig(max_workers=1))\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4595fb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV \n",
    "def export_ragas_results_to_csv(result, llm_model, embedding_model, num_chunks, evaluation_llm, filename):\n",
    "    \"\"\"Export RAGAS evaluation results to CSV with metadata\"\"\"\n",
    "\n",
    "    results_data = {\n",
    "        'llm_model': llm_model,\n",
    "        'embedding_model': embedding_model,\n",
    "        'num_chunks': num_chunks,\n",
    "        'evaluation_llm': evaluation_llm,\n",
    "        'timestamp': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    if hasattr(result, '_scores_dict'):\n",
    "        for metric_name, score in result._scores_dict.items():\n",
    "            results_data[metric_name] = np.mean(score) if isinstance(score, list) else score\n",
    "    else:\n",
    "        for attr in dir(result):\n",
    "            if not attr.startswith('_') and hasattr(result, attr):\n",
    "                value = getattr(result, attr)\n",
    "                if isinstance(value, (int, float, list)):\n",
    "                    results_data[attr] = np.mean(value) if isinstance(value, list) else value\n",
    "    \n",
    "    results_df = pd.DataFrame([results_data])\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    file_exists = os.path.exists(filename)\n",
    "    \n",
    "    results_df.to_csv(filename, mode='a', header=not file_exists, index=False)\n",
    "    \n",
    "    print(f\"Results exported to: {filename}\")\n",
    "    return results_df\n",
    "\n",
    "# Configuration\n",
    "llm_model = \"gpt-4.1-nano\" \n",
    "embedding_model = \"text-embedding-3-small\"  \n",
    "num_chunks = 10\n",
    "evaluation_llm = \"gpt-4.1-mini\"  \n",
    "\n",
    "results_df = export_ragas_results_to_csv(result=result, llm_model=llm_model, embedding_model=embedding_model, num_chunks=num_chunks,\n",
    "                                            evaluation_llm=evaluation_llm, filename=\"baseline_results/baseline_evaluation_results.csv\")\n",
    "\n",
    "print(\"\\nExported data:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6a6ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_results = []\n",
    "for i, sample in enumerate(evaluation_dataset.samples):\n",
    "    sample_result = {\n",
    "        'question': sample.user_input,\n",
    "        'answer': sample.response,\n",
    "        'reference': sample.reference,\n",
    "        'retrieved_contexts': sample.retrieved_contexts\n",
    "    }\n",
    "    \n",
    "    for metric_name in result._scores_dict.keys():\n",
    "        sample_result[f'{metric_name}_score'] = result._scores_dict[metric_name][i]\n",
    "    \n",
    "    detailed_results.append(sample_result)\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_results)\n",
    "print(detailed_df.head())\n",
    "detailed_df.to_csv(\"baseline_results/rag_evaluation_dataset_gpt_4.1_nano_baseline10_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676a9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_failure_df = pd.read_csv(\"ragas_results2/rag_evaluation_dataset_o4_mini10chunks_results.csv\")\n",
    "ragas_failure_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabd27ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sample with lowest answer_relevancy_score\n",
    "lowest_idx = ragas_failure_df['answer_relevancy_score'].idxmin()\n",
    "lowest_sample = ragas_failure_df.loc[lowest_idx]\n",
    "\n",
    "print(f\"Lowest score: {lowest_sample['answer_relevancy_score']:.4f}\")\n",
    "print(f\"\\nQuestion: {lowest_sample['question']}\")\n",
    "print(f\"\\nSystem Answer: \\n\\n{lowest_sample['answer']}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"\\nReference: \\n\\n{lowest_sample['reference']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52647623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 3 worst faithfulness scores and print their system answers\n",
    "worst_indices = ragas_failure_df['faithfulness_score'].nsmallest(3).index.tolist()\n",
    "\n",
    "print(\"SYSTEM ANSWERS FOR THE 3 WORST FAITHFULNESS SCORES:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for rank, idx in enumerate(worst_indices, 1):\n",
    "    sample = ragas_failure_df.loc[idx]\n",
    "    \n",
    "    print(f\"\\nRANK {rank} WORST FAITHFULNESS SCORE\")\n",
    "    print(f\"Index: {idx}\")\n",
    "    print(f\"Faithfulness score: {sample['faithfulness_score']:.4f}\")\n",
    "    print(f\"Question: {sample['question']}\")\n",
    "    print(f\"\\nSystem Answer:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(sample['answer'])\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e962ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find sample with lowest faithfulness_score\n",
    "lowest_faith_idx = ragas_failure_df['faithfulness_score'].idxmin()\n",
    "lowest_faith_sample = ragas_failure_df.loc[lowest_faith_idx]\n",
    "\n",
    "print(f\"Lowest faithfulness score: {lowest_faith_sample['faithfulness_score']:.4f}\")\n",
    "print(f\"\\nQuestion: {lowest_faith_sample['question']}\")\n",
    "print(f\"\\nSystem Answer: \\n\\n {lowest_faith_sample['answer']}\")\n",
    "print(\"---------------------------------------------------------\")\n",
    "print(f\"\\nContext: \\n\\n {lowest_faith_sample['retrieved_contexts']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8264dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse sentence by sentence impact on faithfulness\n",
    "\n",
    "import ast\n",
    "import re\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  \n",
    "\n",
    "def split_into_sentences(text):\n",
    "    \"\"\"Enhanced sentence splitting that handles bullets, lists, and structured text\"\"\"\n",
    "    \n",
    "    # First, split by bullet points and numbered lists\n",
    "    bullet_pattern = r'(?=\\n\\s*[-•*]\\s+|^\\s*[-•*]\\s+|\\n\\s*\\d+\\.\\s+|^\\s*\\d+\\.\\s+)'\n",
    "    parts = re.split(bullet_pattern, text.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for part in parts:\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "            \n",
    "        # If it's a bullet point or numbered item, treat as one sentence\n",
    "        if re.match(r'^\\s*[-•*]\\s+|^\\s*\\d+\\.\\s+', part):\n",
    "            sentences.append(part)\n",
    "        else:\n",
    "            # Split regular text by sentence-ending punctuation\n",
    "            sent_parts = re.split(r'(?<=[.!?])\\s+', part)\n",
    "            sentences.extend([s.strip() for s in sent_parts if s.strip()])\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "lowest_faith_idx = ragas_failure_df['faithfulness_score'].idxmin()\n",
    "lowest_faith_sample = ragas_failure_df.loc[lowest_faith_idx]\n",
    "\n",
    "print(f\"Lowest faithfulness score: {lowest_faith_sample['faithfulness_score']:.4f}\")\n",
    "print(f\"Question: {lowest_faith_sample['question']}\")\n",
    "print(f\"System Answer: {lowest_faith_sample['answer']}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Test the function first\n",
    "sentences = split_into_sentences(lowest_faith_sample['answer'])\n",
    "print(f\"Number of sentences found: {len(sentences)}\")\n",
    "for i, sent in enumerate(sentences, 1):\n",
    "    print(f\"{i}: {sent[:100]}{'...' if len(sent) > 100 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PROCEEDING WITH ANALYSIS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = []\n",
    "current_answer = \"\"\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "    current_answer = sentence if i == 0 else current_answer + \" \" + sentence\n",
    "    \n",
    "    test_data = [{\n",
    "        \"user_input\": lowest_faith_sample['question'],\n",
    "        \"response\": current_answer,\n",
    "        \"retrieved_contexts\": ast.literal_eval(lowest_faith_sample['retrieved_contexts'])\n",
    "    }]\n",
    "    \n",
    "    test_dataset = EvaluationDataset.from_pandas(pd.DataFrame(test_data))\n",
    "    faith_result = evaluate(test_dataset, [Faithfulness()], \n",
    "                           LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\")), \n",
    "                           LangchainEmbeddingsWrapper(embeddings))\n",
    "    \n",
    "    score = faith_result['faithfulness'][0]\n",
    "    results.append({'sentence_num': i + 1, 'faithfulness_score': score, 'sentence_added': sentence})\n",
    "    print(f\"Sentence {i+1}: {score:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"INCREMENTAL ANALYSIS:\")\n",
    "\n",
    "biggest_drop = 0\n",
    "problematic_sentence_idx = None\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    if i == 0:\n",
    "        print(f\"Sentence 1: {result['faithfulness_score']:.4f} (baseline)\")\n",
    "    else:\n",
    "        change = result['faithfulness_score'] - results[i-1]['faithfulness_score']\n",
    "        print(f\"Sentence {result['sentence_num']}: {result['faithfulness_score']:.4f} (change: {change:+.4f})\")\n",
    "        \n",
    "        if change < biggest_drop:\n",
    "            biggest_drop = change\n",
    "            problematic_sentence_idx = i\n",
    "            \n",
    "        if change < -0.05:\n",
    "            print(f\"  ⚠️  ISSUE: {result['sentence_added']}\")\n",
    "\n",
    "retrieved_contexts = ast.literal_eval(lowest_faith_sample['retrieved_contexts'])\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"RETRIEVED CONTEXTS:\")\n",
    "print(f\"Number of contexts: {len(retrieved_contexts)}\")\n",
    "\n",
    "if problematic_sentence_idx is not None:\n",
    "    print(f\"Most problematic sentence (drop: {biggest_drop:.4f}):\")\n",
    "    print(f\"{results[problematic_sentence_idx]['sentence_added']}\")\n",
    "\n",
    "print(f\"\\n{'-'*40}\")\n",
    "for i, context in enumerate(retrieved_contexts, 1):\n",
    "    print(f\"CONTEXT {i}: {context}\")\n",
    "    print(f\"{'-'*40}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f268bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed faithfulness analysis\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  \n",
    "\n",
    "def get_detailed_faithfulness_sync(question, answer, contexts):\n",
    "    \n",
    "    # Create a custom version that exposes intermediate results\n",
    "    class VerboseFaithfulness(Faithfulness):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.last_statements = []\n",
    "            self.last_verdicts = []\n",
    "            \n",
    "        async def _ascore(self, row, callbacks):\n",
    "            # Generate statements\n",
    "            statements_result = await self._create_statements(row, callbacks)\n",
    "            self.last_statements = statements_result.statements\n",
    "            \n",
    "            print(f\"Generated {len(self.last_statements)} statements:\")\n",
    "            for i, stmt in enumerate(self.last_statements, 1):\n",
    "                print(f\"{i}: {stmt}\")\n",
    "            \n",
    "            if not self.last_statements:\n",
    "                return float('nan')\n",
    "            \n",
    "            # Get verdicts\n",
    "            verdicts_result = await self._create_verdicts(row, self.last_statements, callbacks)\n",
    "            self.last_verdicts = verdicts_result.statements\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(\"DETAILED VERDICT ANALYSIS:\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            faithful_count = 0\n",
    "            for i, verdict in enumerate(self.last_verdicts, 1):\n",
    "                status = \"✅ FAITHFUL\" if verdict.verdict == 1 else \"❌ UNFAITHFUL\"\n",
    "                print(f\"\\nStatement {i}: {status}\")\n",
    "                print(f\"Text: {verdict.statement}\")\n",
    "                print(f\"Reason: {verdict.reason}\")\n",
    "                print(f\"Verdict: {verdict.verdict}\")\n",
    "                print(\"-\" * 60)\n",
    "                \n",
    "                if verdict.verdict == 1:\n",
    "                    faithful_count += 1\n",
    "            \n",
    "            score = self._compute_score(verdicts_result)\n",
    "            print(f\"\\nFINAL FAITHFULNESS SCORE: {score:.4f}\")\n",
    "            print(f\"({faithful_count}/{len(self.last_verdicts)} statements faithful)\")\n",
    "            \n",
    "            return score\n",
    "    \n",
    "    # Create the test data\n",
    "    test_data = [{\n",
    "        \"user_input\": question,\n",
    "        \"response\": answer,\n",
    "        \"retrieved_contexts\": contexts\n",
    "    }]\n",
    "    \n",
    "    # Run the verbose analysis\n",
    "    verbose_metric = VerboseFaithfulness()\n",
    "    test_dataset = EvaluationDataset.from_pandas(pd.DataFrame(test_data))\n",
    "    \n",
    "    result = evaluate(test_dataset, [verbose_metric], \n",
    "                     LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4.1-mini\")), \n",
    "                     LangchainEmbeddingsWrapper(embeddings))\n",
    "    \n",
    "    return verbose_metric.last_statements, verbose_metric.last_verdicts, result['faithfulness'][0]\n",
    "\n",
    "# Analyze the problematic response with detailed reasoning\n",
    "print(\"ANALYZING PROBLEMATIC RESPONSE WITH DETAILED REASONING:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use the original question, answer, and contexts from lowest_faith_sample\n",
    "contexts = ast.literal_eval(lowest_faith_sample['retrieved_contexts'])\n",
    "statements, verdicts, final_score = get_detailed_faithfulness_sync(\n",
    "    lowest_faith_sample['question'],        # Original question\n",
    "    lowest_faith_sample['answer'],          # Original answer \n",
    "    contexts                               # Original contexts\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY OF UNFAITHFUL STATEMENTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "unfaithful_statements = []\n",
    "for i, verdict in enumerate(verdicts, 1):\n",
    "    if verdict.verdict == 0:\n",
    "        unfaithful_statements.append((i, verdict))\n",
    "        print(f\"\\n❌ Statement {i}: UNFAITHFUL\")\n",
    "        print(f\"Text: {verdict.statement}\")\n",
    "        print(f\"Reason: {verdict.reason}\")\n",
    "\n",
    "if not unfaithful_statements:\n",
    "    print(\"No unfaithful statements found!\")\n",
    "else:\n",
    "    print(f\"\\n{len(unfaithful_statements)} out of {len(verdicts)} statements were unfaithful\")\n",
    "    print(f\"This resulted in a faithfulness score of {final_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
